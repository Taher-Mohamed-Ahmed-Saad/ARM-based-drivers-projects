{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taher-Mohamed-Ahmed-Saad/ARM-based-drivers-projects/blob/master/Evaluate_ADB_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This Notebook for Running the ADB Project Phase 2"
      ],
      "metadata": {
        "id": "Yj9-3U--Krvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook is divided into two main parts, each focusing on different database sizes:**\n",
        "- **Part 1: Database Size 10K**\n",
        "  - Initiate a new database and insert vectors into it.\n",
        "  - Retrieve vectors from the database.\n",
        "  - Ensure that the insertion time for this database does not exceed 5 minutes.\n",
        "  - Allow flexible RAM usage during insertion but ensure it stays within Google Colab limits.\n",
        "  - Evaluate retrieval time and accuracy.\n",
        "  - Ensure that the peak RAM usage for retrieval does not exceed 5 MB.\n",
        "\n",
        "- **Part 2: Database Sizes 100K and More**\n",
        "  - Generate database vectors using a random seed (refer to the provided code).\n",
        "  - You have generate the database and its index before the submission.\n",
        "  - Implement a VecDB class that loads the pre-generated database, including the index, and retrieves vectors, to load the generated database.\n",
        "  - Evaluate retrieval time and accuracy for different database sizes.\n",
        "  - The Peak RAM usage for the retrieval should not exceed\n",
        "    - For 100 K --> 10 MB\n",
        "    - For 1 M --> 25 MB\n",
        "    - For 5 M --> 75 MB\n",
        "    - For 10 M --> 150 MB\n",
        "    - For 15 M --> 225 MB\n",
        "    - For 20 M --> 300 MB\n",
        "\n",
        "**This notebook is structured into two parts:**\n",
        "\n",
        "- **Part 1 - Modifiable Cells:**\n",
        "This section contains cells that teams are allowed to modify. The modification are only variables and to be submitted during the project's final phase. They are\n",
        "  - GitHub repository link (including PAT token).\n",
        "  - Database (DB) variables, providing the path to the directory or file for loading existing databases and indexes (refer to provided code to see how).\n",
        "\n",
        "- **Part 2 - Non-Modifiable Cells:** This section must not be modified by any team. It includes essential setup and evaluation code. Ensure that the notebook runs smoothly by providing the required inputs in Part 1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "hV2Nc_f8Mbqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Modifiable Cells"
      ],
      "metadata": {
        "id": "C4EV_xB6Kw17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course each team will provide different github repo link\n",
        "Should include PAT token to enable me to download"
      ],
      "metadata": {
        "id": "AODP-iztLtBV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCR6Z8ABxE3w",
        "outputId": "ae43d1bc-5993-4f7d-8430-1b5dac87afb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Adb'...\n",
            "Host key verification failed.\r\n",
            "fatal: Could not read from remote repository.\n",
            "\n",
            "Please make sure you have the correct access rights\n",
            "and the repository exists.\n"
          ]
        }
      ],
      "source": [
        "!git clone git@github.com:kikohafez78/Adb.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teams are required to provide unique paths for the generated databases of sizes 1M, 5M, 10M, 15M, and 20M. Follow these steps to submit the databases:\n",
        "\n",
        "- Once you have the database and index ready, zip the necessary folders/files.\n",
        "- Upload the zip file to Google Drive.\n",
        "- Ensure the file is shareable with \"anyone with the link.\"\n",
        "- Obtain the zip file link (e.g., https://drive.google.com/file/d/1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah/view?usp=drive_link).\n",
        "- Extract the zip file ID (e.g., 1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah).\n",
        "- Place the ID in the designated variable (to be submitted during the project final phase).\n",
        "- The code will automatically download the zip file and unzip it inside this directory.\n",
        "- Provide the local PATH for each database to be passed to the initializer for automatic loading of the database and index (to be submitted during the project final phase)."
      ],
      "metadata": {
        "id": "UsUXWYom6xRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEAM_NUMBER = 1\n",
        "GDRIVE_ID_DB_100K = \"14Gp_C3LxLuYIyF-zL5q-xFXKQ8KOFtZp\"\n",
        "GDRIVE_ID_DB_1M = \"1XbEP6sU0k0UbuPcQLkHJP7cdPtebpsbD\"\n",
        "GDRIVE_ID_DB_5M = \"1DX0tw9YDlvRthjMq3LQ6_aUyp3BvTC1r\"\n",
        "GDRIVE_ID_DB_10M = \"1zHVuY6FfgXvP94NZKIHkViQpeD6et0kb\" # my10M\n",
        "GDRIVE_ID_DB_15M = \"1sPIgNxIuNDUUBnTvAuPMLryd1mqmgwV3\"\n",
        "GDRIVE_ID_DB_20M = \"1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah\"\n",
        "PATH_DB_100K = \"saved_db_100k.csv\"\n",
        "PATH_DB_1M = \"saved_db_1m.csv\"\n",
        "PATH_DB_5M = \"saved_db_5m.csv\"\n",
        "PATH_DB_10M = \"saved_db_10m.csv\"\n",
        "PATH_DB_15M = \"saved_db_15m.csv\"\n",
        "PATH_DB_20M = \"saved_db_20m.csv\""
      ],
      "metadata": {
        "id": "kK46_ZVe5L3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These two varaible I'll change while running in on the discussion"
      ],
      "metadata": {
        "id": "0LGLg01fsujm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY_SEED_NUMBER = 10\n",
        "DB_SEED_NUMBER = 20"
      ],
      "metadata": {
        "id": "G44iH6jnObEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that the project submission will include these\n",
        "- TEAM_NUMBER\n",
        "- Github clone link\n",
        "- GDRIVE_ID_DB_100K\n",
        "- GDRIVE_ID_DB_1M\n",
        "- GDRIVE_ID_DB_5M\n",
        "- GDRIVE_ID_DB_10M\n",
        "- GDRIVE_ID_DB_15M\n",
        "- GDRIVE_ID_DB_20M\n",
        "- PATH_DB_100K\n",
        "- PATH_DB_1M\n",
        "- PATH_DB_5M\n",
        "- PATH_DB_10M\n",
        "- PATH_DB_15M\n",
        "- PATH_DB_20M <br>\n",
        "- And for sure the project document that describes what you did"
      ],
      "metadata": {
        "id": "kWaZ-ByWOIcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: No edits from here\n",
        "#### You can't edit this part, and neither me.\n",
        "#### Note: Maybe I can edit if there is a major bug"
      ],
      "metadata": {
        "id": "hzFTOecwu8wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd sematic_search_DB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqujj7tYTA1l",
        "outputId": "42e938f8-0272-44fd-b8fd-efae9b3b30db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sematic_search_DB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell to run any additional requirement that your code need <br>\n"
      ],
      "metadata": {
        "id": "yJmXzFdisD7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory-profiler >> log.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaPjq2hMqd20",
        "outputId": "1533a2ff-412e-471c-a715-35e813011050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell to download the zip files and unzip them here."
      ],
      "metadata": {
        "id": "lG0DALR498__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown $GDRIVE_ID_DB_100K -O saved_db_100k.zip\n",
        "!gdown $GDRIVE_ID_DB_1M -O saved_db_1m.zip\n",
        "!gdown $GDRIVE_ID_DB_5M -O saved_db_5m.zip\n",
        "!gdown $GDRIVE_ID_DB_10M -O saved_db_10m.zip\n",
        "!gdown $GDRIVE_ID_DB_15M -O saved_db_15m.zip\n",
        "!gdown $GDRIVE_ID_DB_20M -O saved_db_20m.zip\n",
        "!unzip saved_db_100k.zip\n",
        "!unzip saved_db_1m.zip\n",
        "!unzip saved_db_5m.zip\n",
        "!unzip saved_db_10m.zip\n",
        "!unzip saved_db_15m.zip\n",
        "!unzip saved_db_20m.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSv2z0PVp6HA",
        "outputId": "cb0ea4d0-f938-4a53-c4ad-170978e8bf27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14Gp_C3LxLuYIyF-zL5q-xFXKQ8KOFtZp\n",
            "To: /content/sematic_search_DB/saved_db_100k.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 74.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XbEP6sU0k0UbuPcQLkHJP7cdPtebpsbD\n",
            "To: /content/sematic_search_DB/saved_db_1m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 69.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DX0tw9YDlvRthjMq3LQ6_aUyp3BvTC1r\n",
            "To: /content/sematic_search_DB/saved_db_5m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 93.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Jho9rair77eWdA5-iI_qDT2spJNesuDe\n",
            "To: /content/sematic_search_DB/saved_db_10m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 57.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sPIgNxIuNDUUBnTvAuPMLryd1mqmgwV3\n",
            "To: /content/sematic_search_DB/saved_db_15m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 133MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah\n",
            "To: /content/sematic_search_DB/saved_db_20m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 50.2MB/s]\n",
            "Archive:  saved_db_100k.zip\n",
            "  inflating: saved_db_100k.csv       \n",
            "Archive:  saved_db_1m.zip\n",
            "  inflating: saved_db_1m.csv         \n",
            "Archive:  saved_db_5m.zip\n",
            "  inflating: saved_db_5m.csv         \n",
            "Archive:  saved_db_10m.zip\n",
            "  inflating: saved_db_10m.csv        \n",
            "Archive:  saved_db_15m.zip\n",
            "  inflating: saved_db_15m.csv        \n",
            "Archive:  saved_db_20m.zip\n",
            "  inflating: saved_db_20m.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from Graph import Graph, Vertex\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "class VecDB():\n",
        "    #TODO:\n",
        "    # 2 Constructors:\n",
        "        # VecDB() followed by db.insert_records(records_dict)\n",
        "        # VecDB(file_path , new_db = False)\n",
        "        # where file path is the path to the binary file having the index\n",
        "        # retrieve(query, top_k)\n",
        "    # L R\n",
        "    tenKparams=(15,17)\n",
        "    hundredKparams=(20,20)\n",
        "\n",
        "\n",
        "    # file path of binary file\n",
        "    def __init__(self, file_path='DBIndex/', new_db=True):\n",
        "        # R= 17, L= 15, alpha = 2, K= 5,\n",
        "        # self.RecordsPerCluster=2\n",
        "        self.RecordsPerCluster=10**4\n",
        "        if (new_db):\n",
        "            #TODO: Check the records per cluster here or at insert records\n",
        "            self.L,self.R = VecDB.tenKparams\n",
        "        else:\n",
        "            self.L,self.R = VecDB.hundredKparams\n",
        "        self.alpha = 2\n",
        "        self.offset = 0\n",
        "        self.IndexPath = file_path\n",
        "        self.DBGraph=None\n",
        "        self.currentfile=0\n",
        "        # if(not new_db):\n",
        "        #     # load graph from binary file\n",
        "        #     #TODO: remove hard coded value\n",
        "        #     self.DBGraph= pickle.load(open(file_path+\"0.bin\", \"rb\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #TODO: CHECK after klam el mo3eed: might need to handle enk t4oof el directory\n",
        "    # records are list of dictionaries containing id and embeddings\n",
        "    def insert_records(self, records):\n",
        "        # might wanna change records per cluster if records are more than 100k\n",
        "        if(len(records)>self.RecordsPerCluster):\n",
        "            # split into clusters of 10k\n",
        "            for i in range(0, len(records), self.RecordsPerCluster):\n",
        "                if(i+self.RecordsPerCluster>len(records)):\n",
        "                    temp = records[i:]\n",
        "                else:\n",
        "                    temp = records[i:(i + self.RecordsPerCluster)]\n",
        "                #TODO: check this line\n",
        "                self.insert_records(temp)\n",
        "                self.currentfile+=1\n",
        "            return\n",
        "\n",
        "        self.DBGraph = self.Initialize_Random_Graph(records)\n",
        "        self.Build_Index()\n",
        "\n",
        "        # handle directory doesn't exist\n",
        "        if not os.path.exists(self.IndexPath):\n",
        "            os.makedirs(self.IndexPath)\n",
        "        # set current file to len of current files in director\n",
        "        self.currentfile+=len(os.listdir(self.IndexPath))\n",
        "        print(\"writing to \",self.IndexPath+str(self.currentfile)+\".bin\")\n",
        "        with open (self.IndexPath+str(self.currentfile)+\".bin\", 'wb') as f:\n",
        "            pickle.dump(self.DBGraph, f)\n",
        "    #TODO:  CHECK return to this later\n",
        "\n",
        "    def load_binary_data(self, binary_file_path):\n",
        "    # Load the data from the binary file\n",
        "        with open(binary_file_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        # offset of last inserted record\n",
        "        self.offset = data.iloc[0][0]\n",
        "        # print(data)\n",
        "        return data\n",
        "\n",
        "    # Initialize a Graph connected randomly from the given records\n",
        "    # records is a list of dictionaries containing id and embeddings\n",
        "    # the function also calls the graph to calculate its medoid\n",
        "    def Initialize_Random_Graph(self, records):\n",
        "        DBGraph=Graph()\n",
        "        # + here is concat\n",
        "        DBdata = [[x[\"id\"]] + x[\"embed\"] for x in records]\n",
        "        # print(len(DBdata))\n",
        "        for row in DBdata:\n",
        "            dataKey = int(row[0])\n",
        "            dataValue= np.array(row[1:],dtype=float)\n",
        "            # normalize 1 time initially\n",
        "            dataValue /= np.linalg.norm(dataValue)\n",
        "            DBGraph.add_vertex(Vertex(dataKey, dataValue))\n",
        "\n",
        "        size= len(DBGraph.verticies)\n",
        "        # if (size <1000):\n",
        "        #     print(\"DBGraph\")\n",
        "        #     for vertex in DBGraph:\n",
        "        #         print(vertex)\n",
        "            # print(\"size is now\",size)\n",
        "        # for vertex in DBGraph:\n",
        "        #     print(vertex)\n",
        "        # print(size)\n",
        "        # add to offset for when writing the next cluster of vertices\n",
        "        if(size==0):\n",
        "            return\n",
        "        self.offset=DBdata[0][0]\n",
        "\n",
        "        if (size==1):\n",
        "            DBGraph.get_Medoid(self.offset)\n",
        "            return DBGraph\n",
        "        # first element in records\n",
        "        #Building Edges\n",
        "        for vertex in DBGraph:\n",
        "            # we want R neighbors\n",
        "            for i in range(self.R):\n",
        "                neighbor= DBGraph.get_vertex(int(random.random()*size) + self.offset)\n",
        "                while(neighbor==vertex):\n",
        "                    neighbor= DBGraph.get_vertex(int(random.random()*size) + self.offset)\n",
        "                DBGraph.add_edge((vertex.key,vertex.value),(neighbor.key,neighbor.value))\n",
        "        # get medoid of graph ie. calculate it if it's not calculated\n",
        "        DBGraph.get_Medoid(self.offset)\n",
        "        return DBGraph\n",
        "\n",
        "    def index_to_distance(self,id, query):\n",
        "            vertex = self.DBGraph.get_vertex(id)\n",
        "            dist = self.get_distance(vertex.value, query)\n",
        "            return dist\n",
        "\n",
        "    def retrive(self,query,k):\n",
        "        query /= np.linalg.norm(query)\n",
        "        # for handling the shape error\n",
        "        if(query.shape[0]==1):\n",
        "            query=query[0]\n",
        "\n",
        "        # top k from all clusters\n",
        "        ClustersResults = []\n",
        "        for idx, filename in enumerate(os.listdir(self.IndexPath)):\n",
        "            filepath = os.path.join(self.IndexPath, filename)\n",
        "\n",
        "            self.DBGraph = pickle.load(open(filepath,\"rb\"))\n",
        "\n",
        "            TopK,_= self.Greedy_Search(self.DBGraph.medoid,query, k)\n",
        "            # distance,index=((self.index_to_distance(k, query), k+(idx* self.offset)) for k in TopK)\n",
        "            # ListToAdd=\n",
        "            # add top k from this cluster to the list\n",
        "            # print(ListToAdd)\n",
        "            ClustersResults.extend([(self.index_to_distance(VertexId, query), VertexId) for VertexId in TopK])\n",
        "            self.offset+=len(self.DBGraph.verticies)\n",
        "            del self.DBGraph\n",
        "\n",
        "        # print(ClustersResults)\n",
        "        # sorts on first element of the tuple (which are the distances)\n",
        "        ClustersResults.sort()\n",
        "        # print(ClustersResults)\n",
        "        # only get ids\n",
        "        ClustersResults = [element[1] for element in ClustersResults[:k]]\n",
        "        return ClustersResults\n",
        "\n",
        "    # gets euclidean distance between 2 vectors\n",
        "    def get_distance(self,v1,v2):\n",
        "        dist = np.dot(v1,v2) #/ (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "        # print(v1.shape, v2.shape)\n",
        "        return 1.0 - dist\n",
        "        # return  np.linalg.norm(v1-v2)\n",
        "\n",
        "\n",
        "    def get_min_dist_Key (self,AnyKeysSet,Query):\n",
        "        # arrKey = list(AnyKeysSet)\n",
        "        # arrEmb = np.array([self.DBGraph.get_vertex(i).value for i in arrKey])\n",
        "\n",
        "        # a_norm = np.linalg.norm(arrEmb, axis=1)\n",
        "        # b_norm = np.linalg.norm(Query)\n",
        "        # dist = (arrEmb @ Query) / (a_norm * b_norm)\n",
        "\n",
        "        # # minDist = np.linalg.norm(arrEmb, axis=1)\n",
        "        # minIndex = np.argmin(dist)\n",
        "        # return arrKey[minIndex], -1\n",
        "\n",
        "        # put to 50 because the max distance is 2 anyway\n",
        "        min_dist=50\n",
        "        min_vertex=None\n",
        "        for vertexKey in AnyKeysSet:\n",
        "            # print(\"vertex\",vertex)\n",
        "            # print(\"Query\",Query[:3])\n",
        "            vertex=self.DBGraph.get_vertex(vertexKey)\n",
        "            dist=self.get_distance(vertex.value,Query)\n",
        "            # print(dist)\n",
        "            if(dist<min_dist):\n",
        "                min_dist=dist\n",
        "                min_vertex=vertex.key\n",
        "        return min_vertex,min_dist\n",
        "\n",
        "\n",
        "    #initially, start is the medoid\n",
        "    # s is a vertex, Query is a vector\n",
        "    # k is a number, L is a number\n",
        "    # TODO: change them to be indices instead of vertices to save ram\n",
        "    def Greedy_Search(self,start,Query, k):\n",
        "        search_List={start.key}\n",
        "        Visited=set()\n",
        "        #TODO: make the visited and the possible frontier set of indices instead of vertices to save ram.\n",
        "        possible_frontier=search_List\n",
        "        Query /= np.linalg.norm(Query)\n",
        "        while possible_frontier != set():\n",
        "            # print('possible_frontier',possible_frontier)\n",
        "            p_star,_= self.get_min_dist_Key(possible_frontier,Query)\n",
        "\n",
        "            # print('pstar',p_star)\n",
        "            # if p_star==None:\n",
        "            #     # break\n",
        "            #     print('frontier: ')\n",
        "            #     for v in possible_frontier:\n",
        "            #         print(v)\n",
        "            #     print(possible_frontier==set())\n",
        "            search_List=search_List.union(self.DBGraph.get_vertex(p_star).neighbors)\n",
        "            Visited.add(p_star)\n",
        "            if(len(search_List)>self.L):\n",
        "                #update search list to retain closes L points to x_q\n",
        "                search_ListL_L=list(search_List)\n",
        "                search_ListL_L.sort(key=lambda x: self.get_distance(self.DBGraph.get_vertex(x).value,Query))\n",
        "                # only maintain L closest points\n",
        "                search_ListL_L=search_ListL_L[:self.L]\n",
        "                search_List=set(search_ListL_L)\n",
        "\n",
        "            possible_frontier=search_List.difference(Visited)\n",
        "\n",
        "        search_ListL_L=list(search_List)\n",
        "        search_ListL_L.sort(key=lambda x: self.get_distance(self.DBGraph.get_vertex(x).value,Query))\n",
        "        # only maintain k closest points\n",
        "        search_ListL_L=search_ListL_L[:k]\n",
        "        search_List=set(search_ListL_L)\n",
        "        # both are vectors of integers\n",
        "        return search_List,Visited\n",
        "\n",
        "    # # Robust pruning\n",
        "    #candidate set is set of integers\n",
        "    def Robust_Prune(self,point,candidate_set,alpha):\n",
        "        # print(candidate_set)\n",
        "        candidate_set=candidate_set.union(point.neighbors)\n",
        "        # candidate_set.difference({point.key}) # changed\n",
        "        candidate_set=candidate_set.difference({point.key}) # changed\n",
        "        point.neighbors=set()\n",
        "        # print(\"candidate_set\", candidate_set)\n",
        "        # while candidate_set not empty\n",
        "        while candidate_set:\n",
        "            p_star,_= self.get_min_dist_Key(candidate_set,point.value)\n",
        "            point.neighbors.add(p_star)\n",
        "            if(len(point.neighbors)==self.R):\n",
        "                break\n",
        "            DummySet=candidate_set.copy()\n",
        "            for candidatePointKey in candidate_set:\n",
        "                candidatePoint=self.DBGraph.get_vertex(candidatePointKey)\n",
        "                # print(alpha * self.get_distance(self.DBGraph.get_vertex(p_star).value,candidatePoint.value), \" <= \", self.get_distance(candidatePoint.value,point.value))\n",
        "                # print(alpha)\n",
        "                # \">=\" condition is reversed as we use cosine similarity (paper uses L2), so higher value means it is closer\n",
        "                if(alpha * self.get_distance(self.DBGraph.get_vertex(p_star).value,candidatePoint.value) <= self.get_distance(candidatePoint.value,point.value)):\n",
        "                    # print(\"HEEEEEEEEEEEEEEEEELPPPPPPPPPPP !!!!!!!!!!!!!!!!!!!!!!\")\n",
        "                    DummySet.remove(candidatePoint.key)\n",
        "            candidate_set=DummySet\n",
        "\n",
        "\n",
        "    def Build_Index(self):\n",
        "\n",
        "        # R = min(R, len(dataset))\n",
        "        self.iterationOverGraph(1) #alpha=1\n",
        "        self.iterationOverGraph(self.alpha) #alpha=2\n",
        "\n",
        "\n",
        "    def iterationOverGraph(self,alpha):\n",
        "        # print('medoid',medoid)\n",
        "        randIndex = list(self.DBGraph.get_vertices())\n",
        "        random.shuffle(randIndex)\n",
        "        # random permutation + sequential graph update\n",
        "        for n in randIndex:\n",
        "            node = self.DBGraph.get_vertex(n)\n",
        "            # print(n)\n",
        "\n",
        "            (_,V) = self.Greedy_Search(self.DBGraph.medoid, node.value, 1) #K=1\n",
        "\n",
        "            self.Robust_Prune(node, V, alpha)\n",
        "\n",
        "            neighbors = node.get_neighbors()\n",
        "\n",
        "            for inbKey in neighbors:\n",
        "\n",
        "                # CHECK : The backward edge is always added\n",
        "                # check here in case we shouldn't add it in all cases ? Might be incorrect?\n",
        "                inb=self.DBGraph.get_vertex(inbKey)\n",
        "                if len(inb.get_neighbors()) > self.R:\n",
        "                    # print(\"inb.get_neighbors()\", inb.get_neighbors())\n",
        "                    U = inb.get_neighbors().union({node.key})\n",
        "                    # inb.add_neighbor(node.key)\n",
        "                    self.Robust_Prune(inb, U, alpha)\n",
        "                else:\n",
        "                    inb.add_neighbor(node.key)"
      ],
      "metadata": {
        "id": "d6i7riYEX-pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# from vec_db import VecDB\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "# from memory_profiler import memory_usage\n",
        "import gc\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    run_time: float\n",
        "    top_k: int\n",
        "    db_ids: List[int]\n",
        "    actual_ids: List[int]\n",
        "\n",
        "results = []\n",
        "to_print_arr = []\n",
        "\n",
        "def run_queries(db, query, top_k, actual_ids, num_runs):\n",
        "    global results\n",
        "    results = []\n",
        "    for _ in range(num_runs):\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrive(query, top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids))\n",
        "    return results\n",
        "\n",
        "def memory_usage_run_queries(args):\n",
        "    global results\n",
        "    # This part is added to calcauate the RAM usage\n",
        "    mem_before = max(memory_usage())\n",
        "    mem = memory_usage(proc=(run_queries, args, {}), interval = 1e-3)\n",
        "    return results, max(mem) - mem_before\n",
        "\n",
        "def evaluate_result(results: List[Result]):\n",
        "    # scores are negative. So getting 0 is the best score.\n",
        "    scores = []\n",
        "    run_time = []\n",
        "    for res in results:\n",
        "        run_time.append(res.run_time)\n",
        "        # case for retireving number not equal to top_k, socre will be the lowest\n",
        "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
        "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
        "            continue\n",
        "        score = 0\n",
        "        for id in res.db_ids:\n",
        "            try:\n",
        "                ind = res.actual_ids.index(id)\n",
        "                if ind > res.top_k * 3:\n",
        "                    score -= ind\n",
        "            except:\n",
        "                score -= len(res.actual_ids)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
        "\n",
        "def get_actual_ids_first_k(actual_sorted_ids, k):\n",
        "    return [id for id in actual_sorted_ids if id < k]"
      ],
      "metadata": {
        "id": "Sg2vfYgeyavn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This to generate 10K database and the query using the seed numbers that will be changed at submissions day"
      ],
      "metadata": {
        "id": "J3bQQzzWlce4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "vectors = rng.random((20*10**6, 70), dtype=np.float32)\n",
        "\n",
        "# rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
        "# query = rng.random((1, 70), dtype=np.float32)\n",
        "\n",
        "# actual_sorted_ids_10k = np.argsort(vectors.dot(query.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query)), axis= 1).squeeze().tolist()[::-1]"
      ],
      "metadata": {
        "id": "82Mb008w5YB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(0,20_000_000, 1_000_000):\n",
        "  pickle.dumps(vectors[i:i+1_000_000],open(f\"vec{i//1_000_000}.bin\",'wb'))\n"
      ],
      "metadata": {
        "id": "OeR3kk-Ln_kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "elfoBqsoqC6F",
        "outputId": "d6c1a482-0410-438f-97c3-1621b2ddf594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    127\u001b[0m   )\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open new DB add 10K then retrieve and evaluate. Then add another 90K (total 100K) then retrieve and evaluate."
      ],
      "metadata": {
        "id": "QiofTQ56l1wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = VecDB()\n",
        "\n",
        "records_dict = [{\"id\": i, \"embed\": list(row)} for i, row in enumerate(vectors)]\n",
        "db.insert_records(records_dict)\n",
        "res = run_queries(db, query, 5, actual_sorted_ids_10k, 1) # one run to make everything fresh and loaded\n",
        "res, mem = memory_usage_run_queries((db, query, 5, actual_sorted_ids_10k, 5)) # actual runs to compute time, and memory\n",
        "eval = evaluate_result(res)\n",
        "to_print = f\"10K\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "to_print_arr.append(to_print)\n",
        "print(to_print)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "broiY85IyDZ6",
        "outputId": "9eb114f1-192c-49c6-c872-442309195c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10K\tscore\t0.0\ttime\t0.95\tRAM\t0.64 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove exsiting varaibles to empty some RAM"
      ],
      "metadata": {
        "id": "TZy7uYKeFQ-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del vectors\n",
        "del query\n",
        "del actual_sorted_ids_10k\n",
        "del records_dict\n",
        "del db\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MreLqUWLn2uX",
        "outputId": "7810a6f5-6adb-4aa6-9540-b19fd7eefc78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code to generate 20M database. The seed (50) will not be changed. Create the same DB and prepare it's files indexes and every related file. <br>\n",
        "Note at the submission I'll not run the insert records. <br>\n",
        "The query istelf will be changed at submissions day but not the DB"
      ],
      "metadata": {
        "id": "rrOlipAOmy9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(50)\n",
        "vectors = rng.random((10**7*2, 70), dtype=np.float32)\n",
        "\n",
        "rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
        "query = rng.random((1, 70), dtype=np.float32)\n",
        "\n",
        "actual_sorted_ids_20m = np.argsort(vectors.dot(query.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query)), axis= 1).squeeze().tolist()[::-1]"
      ],
      "metadata": {
        "id": "c83ybYSKK85G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "18cb6307-2cde-4d60-d678-ad554334698d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1c57b2a62b63>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQUERY_SEED_NUMBER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Team Number\", TEAM_NUMBER)\n",
        "print(\"\\n\".join(to_print_arr))\n",
        "\n",
        "# db = VecDB(file_path = PATH_DB_100K, new_db = False)\n",
        "# actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**5)\n",
        "# res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
        "# res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
        "# eval = evaluate_result(res)\n",
        "# to_print = f\"100K\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "# to_print_arr.append(to_print)\n",
        "# print(to_print)\n",
        "\n",
        "# del db\n",
        "# del actual_ids\n",
        "# del res\n",
        "# del mem\n",
        "# del eval\n",
        "# gc.collect()\n",
        "\n",
        "# db = VecDB(file_path = PATH_DB_1M, new_db = False)\n",
        "# actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6)\n",
        "# res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
        "# res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
        "# eval = evaluate_result(res)\n",
        "# to_print = f\"1M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "# to_print_arr.append(to_print)\n",
        "# print(to_print)\n",
        "\n",
        "# del db\n",
        "# del actual_ids\n",
        "# del res\n",
        "# del mem\n",
        "# del eval\n",
        "# gc.collect()\n",
        "\n",
        "# db = VecDB(file_path = PATH_DB_5M, new_db = False)\n",
        "# actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6*5)\n",
        "# res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
        "# res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
        "# eval = evaluate_result(res)\n",
        "# to_print = f\"5M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "# to_print_arr.append(to_print)\n",
        "# print(to_print)\n",
        "\n",
        "# del db\n",
        "# del actual_ids\n",
        "# del res\n",
        "# del mem\n",
        "# del eval\n",
        "# gc.collect()\n",
        "\n",
        "db = VecDB(file_path = PATH_DB_10M, new_db = False)\n",
        "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6*10)\n",
        "res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
        "res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
        "eval = evaluate_result(res)\n",
        "to_print = f\"10M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "to_print_arr.append(to_print)\n",
        "print(to_print)\n",
        "\n",
        "del db\n",
        "del actual_ids\n",
        "del res\n",
        "del mem\n",
        "del eval\n",
        "gc.collect()\n",
        "\n",
        "# db = VecDB(file_path = PATH_DB_15M, new_db = False)\n",
        "# actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6*15)\n",
        "# res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
        "# res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
        "# eval = evaluate_result(res)\n",
        "# to_print = f\"15M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "# to_print_arr.append(to_print)\n",
        "# print(to_print)\n",
        "\n",
        "# del db\n",
        "# del actual_ids\n",
        "# del res\n",
        "# del mem\n",
        "# del eval\n",
        "# gc.collect()\n",
        "\n",
        "# db = VecDB(file_path = PATH_DB_20M, new_db = False)\n",
        "# actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6*20)\n",
        "# res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
        "# res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
        "# eval = evaluate_result(res)\n",
        "# to_print = f\"20M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "# to_print_arr.append(to_print)\n",
        "# print(to_print)\n",
        "\n",
        "# del db\n",
        "# del actual_ids\n",
        "# del res\n",
        "# del mem\n",
        "# del eval\n",
        "# gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzkAn4AF19Jl",
        "outputId": "a157e5fa-6f4c-4bbc-fd0e-be9208d79750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team Number 1\n",
            "10K\tscore\t0.0\ttime\t0.95\tRAM\t0.64 MB\n",
            "100K\tscore\t-364888.0\ttime\t6.84\tRAM\t0.90 MB\n",
            "1M\tscore\t-3647735.0\ttime\t7.03\tRAM\t0.08 MB\n",
            "5M\tscore\t-18238226.0\ttime\t7.67\tRAM\t0.03 MB\n",
            "10M\tscore\t-36480322.0\ttime\t7.60\tRAM\t0.00 MB\n",
            "15M\tscore\t-54718935.0\ttime\t7.04\tRAM\t0.00 MB\n",
            "20M\tscore\t-72953015.0\ttime\t7.38\tRAM\t0.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Team Number\", TEAM_NUMBER)\n",
        "print(\"\\n\".join(to_print_arr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt1_7ihfB37Z",
        "outputId": "84af1974-9358-40ee-e02d-78bbc17d0516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team Number 1\n",
            "10K\tscore\t0.0\ttime\t0.95\tRAM\t0.64 MB\n",
            "100K\tscore\t-364888.0\ttime\t6.84\tRAM\t0.90 MB\n",
            "1M\tscore\t-3647735.0\ttime\t7.03\tRAM\t0.08 MB\n",
            "5M\tscore\t-18238226.0\ttime\t7.67\tRAM\t0.03 MB\n",
            "10M\tscore\t-36480322.0\ttime\t7.60\tRAM\t0.00 MB\n",
            "15M\tscore\t-54718935.0\ttime\t7.04\tRAM\t0.00 MB\n",
            "20M\tscore\t-72953015.0\ttime\t7.38\tRAM\t0.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YneWf9F0oS2J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}